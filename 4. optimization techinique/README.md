# 4. Optimization Techniques
### Learning Rate Schedules: Adjust learning rates during training (e.g., exponential decay).
### Gradient Descent Variants:
#### SGD
#### Adam
#### RMSProp
### Regularization:
#### L1/L2 Regularization
#### Dropout
#### Early Stopping